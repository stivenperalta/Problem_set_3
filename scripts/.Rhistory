method = "glmnet",
trControl = ctrl,
family = "binomial",
tuneGrid =expand.grid(alpha =seq(0,1,0.1),lambda = seq(0.1,.2,length.out =20)))
mylogit_en
set.seed(1410)
mylogit_knn <- train(Default~duration+amount+installment+age+
history.buena+history.mala+
purpose.auto_nuevo+purpose.auto_usado+purpose.bienes+purpose.educacion+
foreign.extranjero+
+rent.TRUE,
data = train,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k=c(3,5,7,9,11)))
mylogit_knn
#pi(Y=1)
#pi(Y+1)= (sumatoria 1[Y=1])/N
#primero vamos a encontrar la proporción de 1s que suceden en la muestra
#quiero estimar Pr(Default="Si")
#prior, contar el numero de personas que habian hecho default y lo divido por la muestra total
#dim(train)[1] es el número de observacines en la muestra
p1<-sum(train$Default.Si)/dim(train)[1]
p1
#necesitamos las medias de X|Y=1 y la media de X|Y=0 (de cada particion)
#media de la duración (predictor) para los que default es 1
mu1<-mean(train$duration[train$Default.Si==1])
mu1
mu0<-mean(train$duration[train$Default.Si==0])
mu0
#variance
#LDA hace el supuesto que ambas muestras tienen varianzas iguales
#una estimación de la varianza es la sumatoria de xi- media particular (que obtuvimos en el paso anterior)^2
#luego lo sumamos a través de los grupos (2 en este caso)
#y lo multiplicamos por 1/(N-K) para que sea insesgado
g1<-sum((train$duration[train$Default.Si==1]-mu1)^2)
g0<-sum((train$duration[train$Default.Si==0]-mu0)^2)
sigma<-sqrt((g1+g0)/(dim(train)[1]-2))
sigma
#with the moments, now we can obtain f(X|Y=j) with j=0,1
#quiero estimar f(X|Y=1) y sabemos que es una normal N(media mu, varianza sigma)
#Fórmula de la normal, la debo de escribir en R y reemplazar sigma y mu. pero también lo puedo de hacer con dnorm
f1<-dnorm(test$duration,mean=mu1,sd=sigma)
f0<-dnorm(test$duration,mean=mu0,sd=sigma)
#ya tenemos todos los elementos
post_hand<-f1*p1/(f1*p1+f0*(1-p1))
head(post_hand)
#lo podemos hacer con el paquete mass
p_load("MASS")     # LDA
lda_simple <- lda(Default.Si~duration, data = train)
lda_simple_pred<-predict(lda_simple,test)
names(lda_simple_pred)
posteriors<-data.frame(lda_simple_pred$posterior)
posteriors$hand<-post_hand
head(posteriors)
lda_fit = train(Default~duration+amount+installment+age,
data=train,
method="lda",
trControl = ctrl)
qda_fit=train(Default~duration+amount+installment+age,
data=train,
method="qda",
trControl= ctrl)
qda_fit= train(Default~duration+amount+installment+age,
data=train,
method="qda",
trControl= ctrl)
qda_fit
p_load("klaR")
set.seed(1410)
mylogit_nb <- train(Default~duration+amount+installment+age+
history.buena+history.mala+
purpose.auto_nuevo+purpose.auto_usado+purpose.bienes+purpose.educacion+
foreign.extranjero+
+rent.TRUE,
data = train,
method = "nb",
trControl = ctrl,
tuneGrid=expand.grid(fL=seq(0,10,length.out = 3),
usekernel=TRUE,
adjust=seq(1,10,length.out = 3)))
#Cargar librerías
require("pacman")
p_load(tidyverse)
set.seed(1011)
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
head(credit)
#mutación de factores- queremos crear dummies
credit<-credit %>% mutate(Default=factor(Default,levels=c(0,1),labels=c("No","Si")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
ctrl<- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
set.seed(123)
#si default es un factor, entonces rpart se da cuenta que es un problema de clasificacion
#sino, bota un warning y lo trata como regreison
class_arboles <- train(Default~duration+amount+installment+age+history + purpose+foreign+rent,
data = train,
method = "rpart", #arboles
trControl = ctrl,
tuneLength=100) #100 alphas
class_arboles
#mutación de factores- queremos crear dummies
credit<-credit %>% mutate(Default=factor(Default,levels=c(0,1),labels=c("No","Si")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
#Cargar librerías
require("pacman")
p_load(tidyverse)
set.seed(1011)
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
head(credit)
#mutación de factores- queremos crear dummies
credit<-credit %>% mutate(Default=factor(Default,levels=c(0,1),labels=c("No","Si")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
ctrl<- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
set.seed(123)
#si default es un factor, entonces rpart se da cuenta que es un problema de clasificacion
#sino, bota un warning y lo trata como regreison
class_arboles <- train(Default~duration+amount+installment+age+history + purpose+foreign+rent,
data = train,
method = "rpart", #arboles
trControl = ctrl,
tuneLength=100) #100 alphas
class_arboles
predictTest_arbol <- data.frame(
obs = test$Default,                                    ## observed class labels
predict(class_arboles, newdata = test, type = "prob"),         ## predicted class probabilities
pred = predict(class_arboles, newdata = test, type = "raw")    ## predicted class labels
)
head(predictTest_arbol)
# Accuracy
mean(predictTest_arbol$obs==predictTest_arbol$pred)
p_load("rpart.plot")
prp(class_arboles$finalModel, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,tweak=1.2,clip.facs= TRUE,box.palette = "Greens",compress=FALSE,ycompress = FALSE)
#Bosques
set.seed(123)
class_bosques <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "ranger",
trControl = ctrl,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8),
splitrule = "gini",
min.node.size = c(15,30,45,60))
)
class_bosques <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "ranger",
trControl = ctrl,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8), #cualquier subconjunto es bagging
splitrule = "gini", #parta el arbol a traves de gini
min.node.size = c(15,30,45,60)) #controlamos la profundidad del arbol por el numero de obs minimo
)
class_bosques
predictTest_bosque <- data.frame(
obs = test$Default,                                    ## observed class labels
predict(class_bosques, newdata = test, type = "prob"),         ## predicted class probabilities
pred = predict(class_bosques, newdata = test, type = "raw")    ## predicted class labels
)
# Accuracy
mean(predictTest_bosque$obs==[predictTest_bosque$pred)
# Accuracy
mean(predictTest_bosque$obs==predictTest_bosque$pred)
#AdaBoost
p_load("adabag")
set.seed(123)
class_adaboost <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "AdaBoost.M1",
trControl = ctrl,
tuneGrid=expand.grid(
mfinal = c(50,100,150),
maxdepth = c(1,2,3),
coeflearn = c('Breiman','Freund'))
)
class_adaboost
class_adaboost <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "AdaBoost.M1",
trControl = ctrl,
tuneGrid=expand.grid(
mfinal = c(50,100,150),
maxdepth = c(1,2,3),
coeflearn = c('Breiman','Freund'))
)
#Cargar librerías
require("pacman")
p_load("tidyverse")
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
#Cargar librerías
require("pacman")
p_load("tidyverse")
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
#mutacion de factores
credit<-credit %>% mutate(Default=factor(Default,levels=c(1,0),labels=c("Si","No")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
head(credit)
prop.table(table(credit$Default))
## First, split the training set
set.seed(1011)
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
ctrl<- trainControl(method = "cv", #cross validation
number = 5, #number of folds
classProbs = TRUE, #retorne la probabilidad de clases
savePredictions = T) #retorne predicciones
set.seed(123)
class_ranger <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
metric = "Accuracy", #especificamos que queremos que maximice accuracy
method = "ranger", #bosque
trControl = ctrl,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8), #numero de prodectores
splitrule = "gini", #usando la regla que divide arboles con gini
min.node.size = c(25,50,150,200,250)) #profundidad, número de observaciones en cada nodo (lo mínimo que tienen que tener). mientras más observaciones, menos profundo será el árbol
)
class_ranger
#ahora usamos el F^train (de validacoón cruzada) reemplazamos en los observados de la muestra de entrenamiento (xtrain)= retorna una probabilidad
#usamos la clasificación de bayes 1[p^>0.5]
predictSample <- train   %>%
mutate(hat_default = predict(class_ranger, newdata = train, type = "raw")    ## predicted class labels. type raw= clasificador de bayes. esto dentro de train
)  %>% select(Default,hat_default) #nos quedamos solo con el observado y predicho
head(predictSample)
#miramos resultados
confusionMatrix(data = predictSample$hat_default, reference=predictSample$Default)
# Accuracy (A MANO)
mean(predictSample$Default==predictSample$ hat_default)
#quiero predecir bien fuera de muestra
predictTest <- data.frame(
Default = test$Default,                                    ## observed class labels
hat_default = predict(class_ranger, newdata = test, type = "raw")    ## predicted class labels
)
confusionMatrix(data = predictTest$hat_default, reference=predictTest$Default)
ctrl2<- trainControl(method = "cv",
number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = T)
set.seed(123)
class_ranger_sens <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
metric = "Sens", #antes poníamos accuracy, ahora cambiamos a sens
method = "ranger",
trControl = ctrl2,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8),
splitrule = "gini",
min.node.size = c(25,50,150,200,250))
)
class_ranger_sens
#pero yo lo quiero fuera de muestra
predictTest<- test   %>%
mutate(hat_default_sens = predict(class_ranger_sens, newdata = test, type = "raw")    ## predicted class labels
)  %>% select(Default,hat_default_sens)
confusionMatrix(data = predictTest$hat_default,reference=predictTest$Default)
class_ranger_ROC <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
metric = "ROC", #area bajo la curva
method = "ranger",
trControl = ctrl2,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8),
splitrule = "gini",
min.node.size = c(25,50,150,200,250))
)
class_ranger_ROC
#phat ROC= prob hat(y=si|x)
predictTest <- predictTest  %>%
mutate(class_ROC = predict(class_ranger_ROC, newdata = test, type = "raw"), # predicted class labels
p_hat_ROC=predict(class_ranger_ROC, newdata = test, type = "prob")$Si,         ## predicted class probabilities#
Default_num=ifelse(Default=="No",0,1) #si default es no, reemplazamos por 0, sino, 1
)
head(predictTest)
confusionMatrix(data = predictTest$class_ROC, reference=predictTest$Default)
p_load("pROC")
rfROC <- roc(predictTest$Default, predictTest$p_hat_ROC, levels = rev(levels(predictTest$Default)))
rfROC
FPR<-mean(predictTest$class_ROC[predictTest$Default=="No"]=="No")
TPR<-mean(predictTest$class_ROC[predictTest$Default=="Si"]=="Si")
plot(rfROC)
FPR<-mean(predictTest$class_ROC[predictTest$Default=="No"]=="No")
TPR<-mean(predictTest$class_ROC[predictTest$Default=="Si"]=="Si")
plot(rfROC)
points(x= FPR,
y=TPR,
cex=4, pch=20, col='red')
predictTest <- predictTest  %>%
mutate(class_ROC_2 = factor(ifelse(p_hat_ROC>.2,"Si","No"),levels=c("Si","No"))
)
confusionMatrix(data = predictTest$class_ROC_2, reference=predictTest$Default)
FPR_2<-mean(predictTest$class_ROC_2[predictTest$Default=="No"]=="No")
TPR_2<-mean(predictTest$class_ROC_2[predictTest$Default=="Si"]=="Si")
plot(rfROC, print.auc=TRUE,legacy.axes=TRUE)
## our .5 rule cutoff
points(x= FPR,
y=TPR,
cex=4, pch=20, col='red')
## A .2 rule cutoff
points(x= FPR_2,
y=TPR_2,
cex=4, pch=20, col='blue')
#Closest to top left cutoff
rfThresh <- coords(rfROC, x = "best", best.method = "closest.topleft")
rfThresh
predictTest <- predictTest  %>%
mutate(class_ROC_Thresh = factor(ifelse(p_hat_ROC>rfThresh$threshold,"Si","No"),levels=c("Si","No")) #en base al resultado anterior, pongo el threshold.
)
confusionMatrix(data = predictTest$class_ROC_Thresh, reference=predictTest$Default)
FPR_3<-mean(predictTest$class_ROC_Thresh[predictTest$Default=="No"]=="No")
TPR_3<-mean(predictTest$class_ROC_Thresh[predictTest$Default=="Si"]=="Si")
plot(rfROC, print.auc=TRUE,legacy.axes=TRUE)
plot(rfROC, print.auc=TRUE,legacy.axes=TRUE)
## our .5 rule cutoff
points(x= FPR,
y=TPR,
cex=4, pch=20, col='red')
## A optimal threshold
points(x= FPR_3,
y=TPR_3,
cex=4, pch=20, col='green')
setwd("users/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
setwd("user/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
setwd("Users/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
setwd("User/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret) # Cargar paquetes requeridos
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret) # Cargar paquetes requeridos
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret, dplyr, tidyr, glmnet, pROC) # Cargar paquetes requeridos
#Definir el directorio
path_script<-rstudioapi::getActiveDocumentContext()$path
path_folder<-dirname(path_script)
setwd(path_folder)
getwd()
#vemos que hay en el directorio de stores
dir("../stores")
test<-readRDS("../stores/test_final.rds")
train<-readRDS("../stores/train_final.rds")
#vemos variables
names(train)
#renombramos variable Pobre a pobre
test <- test %>%
rename(pobre = Pobre)
train <- train %>%
rename(pobre=Pobre)
table(train$pobre) #los datos estan desbalanceados
glimpse(train)
#Mutación de factores (tenemos que hacerlo por niveles/levels)
train$pobre <- factor(train$pobre, levels = c("0", "1"), labels = c("No", "Si"))
test$pobre <- factor(test$pobre, levels = c("0", "1"), labels = c("No", "Si"))
#Logit
ctrl<- trainControl(method = "cv", #controla el entrenamiento, la validacion cruzada.
number = 10, #mejor 10. no sirve para dato espaciales
classProbs = TRUE, #probabilidad de las clases en lugar de raw predicciones
verbose=FALSE,
savePredictions = T) #que guarde las predicciones
#hacemos la grilla para los hiperparámetros
hyperparameter_grid <- expand.grid(alpha = seq(0.85, 0.86, 0.01), # iremos variando los valores
lambda = seq(0, 0.02, 0.001)) # iremos variando los valores
colnames(hyperparameter_grid) <- c("alpha", "lambda")
#LOGIT4
set.seed(2023)
logit4 <- train(pobre~Porcentaje_ocupados+ + v.cabecera+ v.cabecera* Jefe_mujer+ cuartos_hog + nper+ npersug
+ d_arriendo + Jefe_mujer+ PersonaxCuarto+ Tipodevivienda
+ Educacion_promedio +edad+ seg_soc+ Nivel_educativo+ Tipo_de_trabajo+otro_trab
+ fondo_pensiones +ocupado, #especifico mi formula. primero utilizaremos todos los predictores "."
data = train,
metric="Accuracy", #metrica de performance
method = "glmnet", #logistic regression with elastic net regularization
trControl = ctrl,
tuneGrid = hyperparameter_grid,
family= "binomial"
)
logit4$bestTune
#Logit4
predictTest_logit4 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit4, type = "prob"),         ## predicted class probabilities
pred = predict(logit4, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
#Logit4
predictTest_logit4 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit4, type = "prob"),         ## predicted class probabilities
pred = predict(logit3, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
#Logit4
predictTest_logit4 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit4, type = "prob"),         ## predicted class probabilities
pred = predict(logit4, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
#Logit3
predictTest_logit3 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit3, newdata=train, type = "prob"),         ## predicted class probabilities
pred = predict(logit3, newdata=train, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
#Logit4
predictTest_logit4 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit4, newdata=train, type = "prob"),         ## predicted class probabilities
pred = predict(logit4, newdata=train, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
head(predictTest_logit4)
confusionMatrix(data = predictTest_logit4$pred, reference=predictTest_logit4$obs)
#LOGIT3
set.seed(2023)
logit3 <- train(pobre~Porcentaje_ocupados+ + v.cabecera+ cuartos_hog + nper
+ d_arriendo + Jefe_mujer+ d_arriendo*Jefe_mujer+ PersonaxCuarto+ Tipodevivienda + Tipodevivienda*Jefe_mujer
+ Educacion_promedio +edad+ edad*Jefe_mujer + seg_soc+ Nivel_educativo+ Nivel_educativo*Jefa_mujer+ Tipo_de_trabajo
+otro_trab +otro_trab*Jefe_mujer
+ fondo_pensiones + fondo_pensiones*Jefe_mujer +ocupado + ocupado*Jefe_mujer, #especifico mi formula. primero utilizaremos todos los predictores "."
data = train,
metric="Accuracy", #metrica de performance
method = "glmnet", #logistic regression with elastic net regularization
trControl = ctrl,
tuneGrid = hyperparameter_grid,
family= "binomial"
)
logit3 <- train(pobre~Porcentaje_ocupados+ + v.cabecera+ cuartos_hog + nper
+ d_arriendo + Jefe_mujer+ d_arriendo*Jefe_mujer+ PersonaxCuarto+ Tipodevivienda + Tipodevivienda*Jefe_mujer
+ Educacion_promedio +edad+ edad*Jefe_mujer + seg_soc+ Nivel_educativo+ Nivel_educativo*Jefe_mujer+ Tipo_de_trabajo
+otro_trab +otro_trab*Jefe_mujer
+ fondo_pensiones + fondo_pensiones*Jefe_mujer +ocupado + ocupado*Jefe_mujer, #especifico mi formula. primero utilizaremos todos los predictores "."
data = train,
metric="Accuracy", #metrica de performance
method = "glmnet", #logistic regression with elastic net regularization
trControl = ctrl,
tuneGrid = hyperparameter_grid,
family= "binomial"
)
#para tune logit3
plot(logit3$results$lambda,
logit3$results$Accuracy,
xlab="lambda",
ylab="Accuracy")
logit3$bestTune
logit3
#Logit3
predictTest_logit3 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit3, type = "prob"),         ## predicted class probabilities
pred = predict(logit3, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
head(predictTest_logit3)
confusionMatrix(data = predictTest_logit3$pred, reference=predictTest_logit3$obs)
#Logit3
predictTest_logit3 <- data.frame(
obs = train$pobre,                    ## observed class labels
predict(logit3, type = "prob"),         ## predicted class probabilities
pred = predict(logit3, type = "raw")    ## predicted class labels (esto luego lo sacamos porque vamos a variar el corte)
)
glimpse(train)
logit3
head(logit3)

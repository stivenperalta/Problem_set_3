q()
Load Packages
#Load Packages
require("pacman")
p_load("tidyverse","stargazer")
options(device = "cairo")  # o "quartz"
p_load("tidyverse","stargazer")
p_load("tidyverse","stargazer")
nlsy = read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/nlsy97.csv')
nlsy = nlsy  %>%   drop_na(educ) #dropea los
spec()
spec(nlsy)
`spec()`
spec()
spec(FALSE)
spec(TRUE)
nlsy = read_csv('https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/nlsy97.csv')
nlsy = nlsy  %>%   drop_na(educ) #dropea los valores faltantes (NA)
View(nlsy)
View(nlsy$educ)
head(nlsy$educ)
sum(nlsy$educ)
summarise(nlsy$educ)
hist(nlsy$educ)
summary(nlsy$educ)
require(pacman)
p_load(tidyverse,fixest, stargazer,knitr,kableExtra,jtools,ggstance,broom,broom.mixed,skimr)
# Ejemplo: Predicting bike rentals with linear regression
ls
# Ejemplo: Predicting bike rentals with linear regression
# Clean environment
rm(list = ls())
# Load relevant packages
require(pacman)
p_load(tidyverse,fixest, stargazer,knitr,kableExtra,jtools,ggstance,broom,broom.mixed,skimr)
# Import dataset
load(url("https://github.com/ignaciomsarmiento/datasets/blob/main/bike.RData?raw=true"))
str(bike)
glimpse(bike)
summary(cnt)
summary(bike$cnt)
install.packages(c("broom", "bslib", "gargle", "googledrive", "googlesheets4"))
#select variables used in he analysis
bike.features.of.interest = c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')
X <- bike %>% select(bike.features.of.interest)
#select variables used in he analysis
library(magrittr)
bike.features.of.interest = c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')
X <- bike %>% select(bike.features.of.interest)
rm(list = ls())
# Load relevant packages
require(pacman)
p_load(tidyverse,fixest, stargazer,knitr,kableExtra,jtools,ggstance,broom,broom.mixed,skimr)
# Import dataset
load(url("https://github.com/ignaciomsarmiento/datasets/blob/main/bike.RData?raw=true"))
bike.features.of.interest = c('season','holiday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'days_since_2011')
X <- bike %>% select(bike.features.of.interest)
#select dependent variable used in he analysis
y <- bike %>%
select(cnt)  %>%
rename(y=cnt)
# Unir variables vectores X y Y
dat = cbind(X, y)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
summary(cars)
summary(cars)
summary(cars)
summary(cars)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
#-skim package-------------------
#Before running the linear model, we check whether all the variables classes are
#either numerical or categorical. A helpful function is skim() from the skimr
#package. This is an alternative to the summary() function in base R, which
#quickly provides a broad overview of a data frame. It handles data of all types,
#dispatching a different set of summary functions based on the types of columns
#in the data frame:
skim(dat)
# Model estimation
mod <- lm(y ~ ., data = dat, x = TRUE)
view(mod)
modglm <- glm(y ~ ., data = dat)
# ver outputs
stargazer(mod,modglm, type="text")
summary(mod)
summ(mod)
ggplot(dat, aes(y = y, x = temp)) +
geom_point() + # add points
stat_smooth(formula = 'y ~ x', method = lm, se = FALSE,
size = 1) +  #fit the linear model in the plot
theme_bw() + #black and white theme
labs(x = "Temperature in Celsius",
y = "Number of bicycles rented",
title = "Predicted values with changes in temperature") # labels
ggplot(dat, aes(y = y, x = temp)) +
geom_point() + # add points
ggplot(dat, aes(y = y, x = temp)) +
geom_point() + # add points
ggplot(dat, aes(y = y, x = temp)) +
geom_point() + # add points
stat_smooth(formula = 'y ~ x', method = lm, se = FALSE,
size = 1) +  #fit the linear model in the plot
theme_bw() + #black and white theme
labs(x = "Temperature in Celsius",
y = "Number of bicycles rented",
title = "Predicted values with changes in temperature") # labels
ggplot(dat, aes(y = y, x = temp)) +
geom_point() + # add points
stat_smooth(formula = 'y ~ x', method = lm, se = FALSE,
size = 1) +  #fit the linear model in the plot
theme_bw() + #black and white theme
labs(x = "Temperature in Celsius",
y = "Number of bicycles rented",
title = "Predicted values with changes in temperature") # labels
# A simple for loop in R
for(i in 1:5) {
print(i)
}
# A simple for loop in R
for(i in 1<5) {
print(i)
}
# A simple for loop in R
for(i in 1>5) {
print(i)
}
#Here
’s a simple example:
# Create a matrix
M <- matrix(c(1:10), nrow = 5)
M
rm(list = ls()) # Limpiar Rstudio
# For syntax
for (value in sequence) {
statements
} # In this syntax, value is the variable that takes on the value of each element in the sequence in turn, and statements are the lines of code executed for each value.
#Here
’s a simple example:
# Create a matrix
M <- matrix(c(1:10), nrow = 5)
M
# Apply the sum function across rows (MARGIN=1)
apply(M, 1, sum)
#Here
’s a simple example:
# Create a list
my_list <- list(a = 1:5, b = 6:10)
my_list
# Apply the sum function to each list element
lapply(my_list, sum)
sessionInfo()
rm(list = ls()) # Limpiar Rstudio
if (!require(pacman)) install.packages("pacman"); require(pacman) # identifica si una librería no está cargada y si es así la empieza a cargar
p_load(ggplot2, rio, tidyverse, skimr, caret) # Cargar varios paquetes al tiempo
library(rvest)
page_1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html")
summary(page_1)
page_1
page_1|> html_elements("p")
page_1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html")
page_1|> html_elements("p")
page_1
page_1|> html_elements("Navugation")
page_1|> html_elements()
# Extraer los párrafos
tablas <- html_sample1 %>%
html_table()
rm(list = ls()) # Limpiar Rstudio
require(pacman)
p_load(ggplot2, rio, tidyverse, skimr, caret, rvest, magrittr) # Cargar varios paquetes al tiempo
sample1 <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html"
browseURL(sample1) # Abrir el URL de la página
html_sample1 <- read_html(sample1) # cargar el html de la página
html_sample1 # ver el html de la página
# Extraer diferentes elementos de la página
# Extraer los títulos
html_sample1 %>%
html_elements("h3") %>%
html_text()
# Extraer los párrafos
html_sample1 %>%
html_elements("p") %>%
html_text()
# Extraer los párrafos
tablas <- html_sample1 %>%
html_table()
tablas
# Extraer tablas ya creadas
tablas <- html_sample1 %>%
html_table() # en los paréntesis selecciono la tabla que quiero
tablas
# Extraer tablas que se deben formar
tabla <- html_sample1 %>%
html_nodes(xpath = "/html/body/div/div/div[2]/div/table")
tabla
# Extraer tablas que se deben formar
tabla <- html_sample1 %>%
html_nodes(xpath = "/html/body/div/div/div[2]/div/table") %>%
html_table()
sample1 %>%
html_table()
html_sample1 %>%
html_table()
table1 <- read_html("https://ignaciomsarmiento.github.io/assets/css/bootstrap.min.css")
table1
view(table1)
as.data.frame(table1)
tabla1 %>%
html_nodes(xpath = "https://ignaciomsarmiento.github.io/assets/css/bootstrap.min.css") %>%
html_table()
tabla1 <- read_html("https://ignaciomsarmiento.github.io/assets/css/bootstrap.min.css") %>%
html_nodes(xpath = "https://ignaciomsarmiento.github.io/assets/css/bootstrap.min.css") %>%
html_table()
tabla1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/page1.html") %>%
html_nodes(xpath = "https://ignaciomsarmiento.github.io/assets/css/bootstrap.min.css") %>%
html_table()
texto <- html_text(nodos)
tabla1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html") %>%
html_table()
tabla1
view(tabla1)
rm(list = ls()) # Limpiar Rstudio
require(pacman)
p_load(ggplot2, rio, tidyverse, skimr, caret, rvest, magrittr) # Cargar varios paquetes al tiempo
rm(list = ls()) # Limpiar Rstudio
options(scipen = 20,  digits=1)
require(pacman)
p_load(ggplot2, rio, tidyverse, skimr, caret, rvest, magrittr) # Cargar varios paquetes al tiempo
rm(list = ls()) # Limpiar Rstudio
options(scipen = 20,  digits=1)
require(pacman)
p_load(ggplot2, rio, tidyverse, skimr, caret, rvest, magrittr) # Cargar varios paquetes al tiempo
table1 <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_1.html") %>%
html_table()
head(tabla1)
head(table1)
title(table1)
names(table1)
as.data.frame(table1)
names(table1)
?for
for (variable in vector) {
?for
d
for?
data_base <- ("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_") %>%
for (i in 1:10) {
datai <- paste0(data_base, i, ".html")
}
data_base <- ("https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_")
head(data_base)
for (i in 1:10) {
datai <- paste0(data_base, i, ".html")
}
# creo un loop para descargar las 10 bases de datos
url_base <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_"
numero_inicial <- 1
numero_final <- 10
for (i in numero_inicial:numero_final) {
url <- paste0(url_base, i, ".html")
pagina <- read_html(url)
}
head(i)
view(i)
paginai <- read_html(url) %>%
html_table()
# creo un loop para descargar las 10 bases de datos
url_base <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/pages/geih_page_"
for (i in 1:10) {
url <- paste0(url_base, i, ".html")
paginai <- read_html(url)
html_table()
}
paginai <- read_html(url)
for (i in 1:10) {
url <- paste0(url_base, i, ".html")
paginai <- read_html(url)
}
for (i in 1:10) {
url <- paste0(url_base, i, ".html")
read_html(url)
}
head(url)
citation()
citation("rvest")
citation("car ")
citation("car")
install.packages(c("cpp11", "dbplyr", "digest", "future", "gargle", "haven", "jsonlite", "lme4", "pkgload", "pROC", "processx", "Rcpp", "readxl", "rmarkdown", "rstudioapi", "shiny", "testthat", "vctrs", "webshot", "xml2"))
q()
# Classification ##########################################################
rm(list = ls())
pacman::p_load(tydiverse)
getwd()
pacman::p_load(tydiverse)
pacman::p_load(tidyverse)
getwd()
getwd()
getwd()
getwd()
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, #gráficas
tidyverse, #limpieza
caret, #modelos
parallel, # conocer los cores de mi pc
doParallel, # maximizar el procesamiento en r en función de los cores de mi pc
ranger,
dplyr, tidyr, glmnet, pROC, randomForest) # Cargar paquetes requeridos
#Definir el directorio
path_script<-rstudioapi::getActiveDocumentContext()$path
path_folder<-dirname(path_script)
setwd(path_folder)
getwd()
rm(path_folder, path_script)
# maximizo el procesamiento de r
detectCores() # detecta los cores del computador
function (x, ...)
registerDoParallel(6) # 6 de 8 cores de mi computador para R
getDoParWorkers() # verifico el número de cores usados por R
# maximizo el procesamiento de r
detectCores() # detecta los cores del computador
registerDoParallel(6) # 6 de 8 cores de mi computador para R
getDoParWorkers() # verifico el número de cores usados por R
#vemos que hay en el directorio de stores
dir("../stores")
#Importamos las bases
test<-readRDS("../stores/test_final.rds")
train<-readRDS("../stores/train_final.rds")
#renombramos variable Pobre a pobre (para poder cargarla en kaggle)
test <- test %>%
rename(pobre = Pobre)
train <- train %>%
rename(pobre=Pobre)
prop.table(table(train$pobre)) #los datos estan desbalanceados
#Mutación de factores (tenemos que hacerlo por niveles/levels)
test$pobre <- factor(test$pobre, levels = c("0", "1"),
labels = c("No", "Si"))
train$pobre <- factor(train$pobre, levels = c("0", "1"),
labels = c("No", "Si"))
#convertimos en factor
train$Regimen_salud <- as.factor(train$Regimen_salud)
test$Regimen_salud <- as.factor(test$Regimen_salud)
# Evalúo la correlación de las variables (Matríz de correlación)
numeric_train <- train %>% select_if(is.numeric) #separamos las numericas
numeric_train <- ungroup(numeric_train) %>% select(-id)
cor_matrix <- cor(numeric_train) #calculamos correlacion
print(cor_matrix)
library(corrplot)
corrplot(cor_matrix, method = "circle", tl.col = "black")
rm(cor_matrix, numeric_train)
# Selecciono las variables para emplear en el modelo
names(train)
summary(train)
# selecciono variables de mayor interés
train <- select(train, c(1:4, 6, 13:14, 16, 17, 19:23, 26, 44, 47, 48, 8, 9))
test <- select(test, c(1:4, 6, 13:14, 16, 17, 19:23, 26, 44, 47, 48, 8, 9))
summary(train)
# convierto la variable pobre en factor y la ajusto con valores de 1 y 0
train$pobre <- as.factor(train$pobre)
test$pobre <- as.factor(test$pobre)
#Creo las bases para poder hacer las predicciones
test_relevant <- test %>%
ungroup() %>%
select(Porcentaje_ocupados, v.cabecera, cuartos_hog, nper,
d_arriendo, Jefe_mujer, PersonaxCuarto, Tipodevivienda,
Educacion_promedio, sexo, edad, seg_soc, Nivel_educativo,
Tipo_de_trabajo, ocupado, IngresoPerCapita, pobre)
train_prueba <- train %>%
ungroup() %>%
select(Porcentaje_ocupados, v.cabecera, cuartos_hog, nper,
d_arriendo, Jefe_mujer, PersonaxCuarto, Tipodevivienda,
Educacion_promedio, sexo, edad, seg_soc, Nivel_educativo,
Tipo_de_trabajo, ocupado, IngresoPerCapita, pobre)
#Balanceo la muestra para nivelar con el valor de menor frecuencia
set.seed(201718234)
down_train <- downSample(x = train[, -ncol(train)],
y = train$pobre)
table(down_train$pobre)
#Balanceo la muestra para nivelar con el valor de mayor frecuencia
set.seed(201718234)
up_train <- upSample(x = train[, -ncol(train)],
y = train$pobre)
table(up_train$pobre)
# Creo los parámetros e hiperparámetros de ajuste del modelo -------------------
ctrl <- trainControl(method = "repeatedcv", repeats = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary)
set.seed(201718234)
# Creo los parámetros e hiperparámetros de ajuste del modelo -------------------
ctrl <- trainControl(method = "repeatedcv",
repeats = 5,
classProbs = TRUE, # guardar probabilidades
summaryFunction = twoClassSummary) # calcular métricas para accuracy

names(lda_simple_pred)
posteriors<-data.frame(lda_simple_pred$posterior)
posteriors$hand<-post_hand
head(posteriors)
lda_fit = train(Default~duration+amount+installment+age,
data=train,
method="lda",
trControl = ctrl)
qda_fit=train(Default~duration+amount+installment+age,
data=train,
method="qda",
trControl= ctrl)
qda_fit= train(Default~duration+amount+installment+age,
data=train,
method="qda",
trControl= ctrl)
qda_fit
p_load("klaR")
set.seed(1410)
mylogit_nb <- train(Default~duration+amount+installment+age+
history.buena+history.mala+
purpose.auto_nuevo+purpose.auto_usado+purpose.bienes+purpose.educacion+
foreign.extranjero+
+rent.TRUE,
data = train,
method = "nb",
trControl = ctrl,
tuneGrid=expand.grid(fL=seq(0,10,length.out = 3),
usekernel=TRUE,
adjust=seq(1,10,length.out = 3)))
#Cargar librerías
require("pacman")
p_load(tidyverse)
set.seed(1011)
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
head(credit)
#mutación de factores- queremos crear dummies
credit<-credit %>% mutate(Default=factor(Default,levels=c(0,1),labels=c("No","Si")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
ctrl<- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
set.seed(123)
#si default es un factor, entonces rpart se da cuenta que es un problema de clasificacion
#sino, bota un warning y lo trata como regreison
class_arboles <- train(Default~duration+amount+installment+age+history + purpose+foreign+rent,
data = train,
method = "rpart", #arboles
trControl = ctrl,
tuneLength=100) #100 alphas
class_arboles
#mutación de factores- queremos crear dummies
credit<-credit %>% mutate(Default=factor(Default,levels=c(0,1),labels=c("No","Si")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
#Cargar librerías
require("pacman")
p_load(tidyverse)
set.seed(1011)
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
head(credit)
#mutación de factores- queremos crear dummies
credit<-credit %>% mutate(Default=factor(Default,levels=c(0,1),labels=c("No","Si")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
ctrl<- trainControl(method = "cv",
number = 5,
classProbs = TRUE,
verbose=FALSE,
savePredictions = T)
set.seed(123)
#si default es un factor, entonces rpart se da cuenta que es un problema de clasificacion
#sino, bota un warning y lo trata como regreison
class_arboles <- train(Default~duration+amount+installment+age+history + purpose+foreign+rent,
data = train,
method = "rpart", #arboles
trControl = ctrl,
tuneLength=100) #100 alphas
class_arboles
predictTest_arbol <- data.frame(
obs = test$Default,                                    ## observed class labels
predict(class_arboles, newdata = test, type = "prob"),         ## predicted class probabilities
pred = predict(class_arboles, newdata = test, type = "raw")    ## predicted class labels
)
head(predictTest_arbol)
# Accuracy
mean(predictTest_arbol$obs==predictTest_arbol$pred)
p_load("rpart.plot")
prp(class_arboles$finalModel, under = TRUE, branch.lty = 2, yesno = 2, faclen = 0, varlen=15,tweak=1.2,clip.facs= TRUE,box.palette = "Greens",compress=FALSE,ycompress = FALSE)
#Bosques
set.seed(123)
class_bosques <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "ranger",
trControl = ctrl,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8),
splitrule = "gini",
min.node.size = c(15,30,45,60))
)
class_bosques <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "ranger",
trControl = ctrl,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8), #cualquier subconjunto es bagging
splitrule = "gini", #parta el arbol a traves de gini
min.node.size = c(15,30,45,60)) #controlamos la profundidad del arbol por el numero de obs minimo
)
class_bosques
predictTest_bosque <- data.frame(
obs = test$Default,                                    ## observed class labels
predict(class_bosques, newdata = test, type = "prob"),         ## predicted class probabilities
pred = predict(class_bosques, newdata = test, type = "raw")    ## predicted class labels
)
# Accuracy
mean(predictTest_bosque$obs==[predictTest_bosque$pred)
# Accuracy
mean(predictTest_bosque$obs==predictTest_bosque$pred)
#AdaBoost
p_load("adabag")
set.seed(123)
class_adaboost <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "AdaBoost.M1",
trControl = ctrl,
tuneGrid=expand.grid(
mfinal = c(50,100,150),
maxdepth = c(1,2,3),
coeflearn = c('Breiman','Freund'))
)
class_adaboost
class_adaboost <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
method = "AdaBoost.M1",
trControl = ctrl,
tuneGrid=expand.grid(
mfinal = c(50,100,150),
maxdepth = c(1,2,3),
coeflearn = c('Breiman','Freund'))
)
#Cargar librerías
require("pacman")
p_load("tidyverse")
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
#Cargar librerías
require("pacman")
p_load("tidyverse")
#Leer los datos
credit <- readRDS(url("https://github.com/ignaciomsarmiento/datasets/blob/main/credit_class.rds?raw=true"))
#mutacion de factores
credit<-credit %>% mutate(Default=factor(Default,levels=c(1,0),labels=c("Si","No")),
history=factor(history,levels=c("good","poor","terrible"),labels=c("buena","mala","terrible")),
foreign=factor(foreign,levels=c("foreign","german"),labels=c("extranjero","aleman")),
purpose=factor(purpose,levels=c("newcar","usedcar","goods/repair","edu", "biz" ),labels=c("auto_nuevo","auto_usado","bienes","educacion","negocios")))
head(credit)
prop.table(table(credit$Default))
## First, split the training set
set.seed(1011)
p_load("caret")
inTrain <- createDataPartition(
y = credit$Default,## La variable dependiente u objetivo
p = .7, ## Usamos 70%  de los datos en el conjunto de entrenamiento
list = FALSE)
train <- credit[ inTrain,]
test  <- credit[-inTrain,]
ctrl<- trainControl(method = "cv", #cross validation
number = 5, #number of folds
classProbs = TRUE, #retorne la probabilidad de clases
savePredictions = T) #retorne predicciones
set.seed(123)
class_ranger <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
metric = "Accuracy", #especificamos que queremos que maximice accuracy
method = "ranger", #bosque
trControl = ctrl,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8), #numero de prodectores
splitrule = "gini", #usando la regla que divide arboles con gini
min.node.size = c(25,50,150,200,250)) #profundidad, número de observaciones en cada nodo (lo mínimo que tienen que tener). mientras más observaciones, menos profundo será el árbol
)
class_ranger
#ahora usamos el F^train (de validacoón cruzada) reemplazamos en los observados de la muestra de entrenamiento (xtrain)= retorna una probabilidad
#usamos la clasificación de bayes 1[p^>0.5]
predictSample <- train   %>%
mutate(hat_default = predict(class_ranger, newdata = train, type = "raw")    ## predicted class labels. type raw= clasificador de bayes. esto dentro de train
)  %>% select(Default,hat_default) #nos quedamos solo con el observado y predicho
head(predictSample)
#miramos resultados
confusionMatrix(data = predictSample$hat_default, reference=predictSample$Default)
# Accuracy (A MANO)
mean(predictSample$Default==predictSample$ hat_default)
#quiero predecir bien fuera de muestra
predictTest <- data.frame(
Default = test$Default,                                    ## observed class labels
hat_default = predict(class_ranger, newdata = test, type = "raw")    ## predicted class labels
)
confusionMatrix(data = predictTest$hat_default, reference=predictTest$Default)
ctrl2<- trainControl(method = "cv",
number = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = T)
set.seed(123)
class_ranger_sens <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
metric = "Sens", #antes poníamos accuracy, ahora cambiamos a sens
method = "ranger",
trControl = ctrl2,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8),
splitrule = "gini",
min.node.size = c(25,50,150,200,250))
)
class_ranger_sens
#pero yo lo quiero fuera de muestra
predictTest<- test   %>%
mutate(hat_default_sens = predict(class_ranger_sens, newdata = test, type = "raw")    ## predicted class labels
)  %>% select(Default,hat_default_sens)
confusionMatrix(data = predictTest$hat_default,reference=predictTest$Default)
class_ranger_ROC <- train(
Default~duration+amount+installment+age+history + purpose+foreign+rent,
data=train,
metric = "ROC", #area bajo la curva
method = "ranger",
trControl = ctrl2,
tuneGrid=expand.grid(
mtry = c(1,2,3,4,5,6,7,8),
splitrule = "gini",
min.node.size = c(25,50,150,200,250))
)
class_ranger_ROC
#phat ROC= prob hat(y=si|x)
predictTest <- predictTest  %>%
mutate(class_ROC = predict(class_ranger_ROC, newdata = test, type = "raw"), # predicted class labels
p_hat_ROC=predict(class_ranger_ROC, newdata = test, type = "prob")$Si,         ## predicted class probabilities#
Default_num=ifelse(Default=="No",0,1) #si default es no, reemplazamos por 0, sino, 1
)
head(predictTest)
confusionMatrix(data = predictTest$class_ROC, reference=predictTest$Default)
p_load("pROC")
rfROC <- roc(predictTest$Default, predictTest$p_hat_ROC, levels = rev(levels(predictTest$Default)))
rfROC
FPR<-mean(predictTest$class_ROC[predictTest$Default=="No"]=="No")
TPR<-mean(predictTest$class_ROC[predictTest$Default=="Si"]=="Si")
plot(rfROC)
FPR<-mean(predictTest$class_ROC[predictTest$Default=="No"]=="No")
TPR<-mean(predictTest$class_ROC[predictTest$Default=="Si"]=="Si")
plot(rfROC)
points(x= FPR,
y=TPR,
cex=4, pch=20, col='red')
predictTest <- predictTest  %>%
mutate(class_ROC_2 = factor(ifelse(p_hat_ROC>.2,"Si","No"),levels=c("Si","No"))
)
confusionMatrix(data = predictTest$class_ROC_2, reference=predictTest$Default)
FPR_2<-mean(predictTest$class_ROC_2[predictTest$Default=="No"]=="No")
TPR_2<-mean(predictTest$class_ROC_2[predictTest$Default=="Si"]=="Si")
plot(rfROC, print.auc=TRUE,legacy.axes=TRUE)
## our .5 rule cutoff
points(x= FPR,
y=TPR,
cex=4, pch=20, col='red')
## A .2 rule cutoff
points(x= FPR_2,
y=TPR_2,
cex=4, pch=20, col='blue')
#Closest to top left cutoff
rfThresh <- coords(rfROC, x = "best", best.method = "closest.topleft")
rfThresh
predictTest <- predictTest  %>%
mutate(class_ROC_Thresh = factor(ifelse(p_hat_ROC>rfThresh$threshold,"Si","No"),levels=c("Si","No")) #en base al resultado anterior, pongo el threshold.
)
confusionMatrix(data = predictTest$class_ROC_Thresh, reference=predictTest$Default)
FPR_3<-mean(predictTest$class_ROC_Thresh[predictTest$Default=="No"]=="No")
TPR_3<-mean(predictTest$class_ROC_Thresh[predictTest$Default=="Si"]=="Si")
plot(rfROC, print.auc=TRUE,legacy.axes=TRUE)
plot(rfROC, print.auc=TRUE,legacy.axes=TRUE)
## our .5 rule cutoff
points(x= FPR,
y=TPR,
cex=4, pch=20, col='red')
## A optimal threshold
points(x= FPR_3,
y=TPR_3,
cex=4, pch=20, col='green')
setwd("users/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
setwd("user/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
setwd("Users/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
setwd("User/jazminegaldos/Documents/Uniandes/02_Ciclo/Big Data/GitHub/repositorios/Problem_set_3/scripts")
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret) # Cargar paquetes requeridos
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret) # Cargar paquetes requeridos
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret, dplyr, tidyr, glmnet, pROC) # Cargar paquetes requeridos
#Definir el directorio
path_script<-rstudioapi::getActiveDocumentContext()$path
path_folder<-dirname(path_script)
setwd(path_folder)
getwd()
#vemos que hay en el directorio de stores
dir("../stores")
hogares<-read_csv("../stores/test_hogares.csv")
personas<-read_csv("../stores/test_personas.csv")
#vemos variables
names(hogares)
names(personas)
#para facilidad cambiamos nombres de variables
hogares <- hogares %>%
rename(
cuartos = P5000,
habitaciones=P5010,
estado=P5090,
amortizacion=P5100,
arriendo_aprox=P5130,
arriendo_real=P5140,
personas_gasto=Npersug
)
personas<-personas %>%
rename(
sexo=P6020,
edad=P6040,
parentesco=P6050,
afil_seg_social=P6090,
tipo_seg_social=P6100,
nivel_educ=P6210,
grado=P6210s1,
actividad=P6240,
tiempo_empr=P6426,
posicion=P6430,
ing_horas_ext=P6510,
ing_otros=P6545,
bonificacion=P6580,
sub_ali=P6585s1,
sub_trans=P6585s2,
sub_fam=P6585s3,
sub_edu=P6585s4,
ing_alim=P6590,
ing_viv=P6600,
transp=P6610,
ing_otro=P6620,
prima_ser=P6630s1,
prima_nav=P6630s2,
prima_vac=P6630s3,
viaticos=P6630s4,
bono_anual=P6630s6,
horas_trab_sem=P6800,
tamano_empr=P6870,
fondo_pens=P6920,
seg_traba=P7040,
horas_seg_trab=P7045,
seg_posicion=P7050,
traba_mas=P7090,
busc_trab_mas=P7110,
disp_trab_mas=P7120,
camb_trab=P7150,
emp_mes=P7160,
busc_trab=P7310,
trab_desocup=P7350,
ing_desocup=P7422,
ing_desocup2=P7472,
ing_arr_pens=P7495,
ing_pens=P7500s2,
ing_pens_alim=P7500s3,
ing_otros_total=P7505,
ing_resid=P7510s1,
ing_fpais=P7510s2,
ing_instit=P7510s3,
ing_inter=P7510s5,
ing_cesantia=P7510s6,
ing_otra_fuente=P7510s7
)
#Creo una variable de pobreza en hogares para poder avanzar
hogares <- hogares %>%
mutate(pobre = rbinom(n(), 1, 0.5))
table(hogares$pobre)
#Mutación de factores (tenemos que hacerlo por niveles/levels)
hogares$pobre <- factor(hogares$pobre, levels = c("0", "1"), labels = c("No", "Si"))
hogares$Dominio<-as.factor(hogares$Dominio)
#Saco Li y personas_gasto por alta correlación
hogares <- hogares %>%
select(-Li, -personas_gasto)
#reemplazo NAs por 0s (solo para armar formulas)
hogares <- replace(hogares, is.na(hogares), 0)
#Logit
ctrl<- trainControl(method = "cv", #controla el entrenamiento, la validacion cruzada.
number = 10, #mejor 10. no sirve para dato espaciales
classProbs = TRUE, #probabilidad de las clases en lugar de raw predicciones
verbose=FALSE,
savePredictions = T) #que guarde las predicciones
set.seed(2023)
#hacemos la grilla para los hiperparámetros
hyperparameter_grid <- expand.grid(alpha = seq(0, 1, 0.1), # iremos variando los valores
lambda = seq(0, 1, 0.1)) # iremos variando los valores
hyperparameter_grid2 <- expand.grid(alpha = seq(0, 1, 0.1), # iremos variando los valores
lambda = seq(0, 1, 0.1),
splitrule="gini")
colnames(hyperparameter_grid) <- c("alpha", "lambda")
colnames(hyperparameter_grid2) <- c("alpha","lambda","splitrule")
logit1 <- train(pobre~Dominio+cuartos+habitaciones+estado+amortizacion+ #especifico mi formula, dejo los que pueden crear multicolinealidad
arriendo_aprox+arriendo_real+Nper+Lp,
data = hogares,
metric="Accuracy",
method = "glmnet",
trControl = ctrl,
tuneGrid = hyperparameter_grid,
family= "binomial"
)
logit2 <- train(pobre~Dominio+cuartos+habitaciones+estado+amortizacion+ #especifico mi formula, dejo los que pueden crear multicolinealidad
arriendo_aprox+arriendo_real+Nper+Lp,
data = hogares,
metric="ROC",
method = "glmnet",
trControl = ctrl,
tuneGrid = hyperparameter_grid2,
family= "binomial"
)
rm(hyperparameter_grid2)
hyperparameter_grid2 <- expand.grid(alpha = seq(0, 1, 0.1), # iremos variando los valores
lambda = seq(0, 1, 0.1),
splitrule="gini")
logit2 <- train(pobre~Dominio+cuartos+habitaciones+estado+amortizacion+ #especifico mi formula, dejo los que pueden crear multicolinealidad
arriendo_aprox+arriendo_real+Nper+Lp,
data = hogares,
metric="ROC",
method = "glmnet",
trControl = ctrl,
tuneGrid = hyperparameter_grid2,
family= "binomial"
)
glimpse(hyperparameter_grid2)
hyperparameter_grid2 <- expand.grid( # iremos variando los valores
lambda = seq(0, 1, 0.1),
splitrule="gini")
logit2 <- train(pobre~Dominio+cuartos+habitaciones+estado+amortizacion+ #especifico mi formula, dejo los que pueden crear multicolinealidad
arriendo_aprox+arriendo_real+Nper+Lp,
data = hogares,
metric="ROC",
method = "glmnet",
trControl = ctrl,
tuneGrid = hyperparameter_grid2,
family= "binomial"
)
logit2 <- train(pobre~Dominio+cuartos+habitaciones+estado+amortizacion+ #especifico mi formula, dejo los que pueden crear multicolinealidad
arriendo_aprox+arriendo_real+Nper+Lp,
data = hogares,
metric="ROC",
method = "glmnet",
trControl = ctrl,
tuneGrid = hyperparameter_grid2,
family= "binomial",
alpha=1
)
View(hyperparameter_grid)
logit2 <- train(pobre~Dominio+cuartos+habitaciones+estado+amortizacion+ #especifico mi formula, dejo los que pueden crear multicolinealidad
arriendo_aprox+arriendo_real+Nper+Lp,
data = hogares,
metric="ROC",
method = "glmnet",
trControl = ctrl,
tuneGrid = hyperparameter_grid,
family= "binomial",
splitrule= "gini"
)
rm(list = ls()) # Limpiar Rstudio
pacman::p_load(ggplot2, tidyverse, caret, dplyr, tidyr, glmnet, pROC) # Cargar paquetes requeridos
#Definir el directorio
path_script<-rstudioapi::getActiveDocumentContext()$path
path_folder<-dirname(path_script)
setwd(path_folder)
getwd()
#vemos que hay en el directorio de stores
dir("../stores")
